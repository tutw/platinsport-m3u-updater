#!/usr/bin/env python3

"""
PlatinSport Acestream Scraper - VERSI√ìN ULTIMATE
================================================
‚úÖ Extrae TODOS los eventos de la p√°gina principal
‚úÖ Mapea cada evento con sus URLs Acestream desde source-list.php
‚úÖ Manejo robusto de errores con retry logic
‚úÖ Generaci√≥n de M3U con informaci√≥n completa
‚úÖ Logging detallado y estad√≠sticas
‚úÖ Validaci√≥n completa de URLs Acestream
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import re
import time
import base64
import logging
from typing import Dict, List, Optional
from collections import defaultdict

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class UltimatePlatinSportScraper:
    """Scraper optimizado que extrae TODOS los enlaces Acestream de eventos"""

    def __init__(self):
        self.base_url = "https://platinsport.com"
        self.session = requests.Session()
        self.retry_count = 3
        self.retry_delay = 2

        # Configurar headers
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                          'AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,'
                      'image/webp,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            # Dejamos que requests maneje gzip/deflate/brotli si hay librer√≠a instalada
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://platinsport.com/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        })

        self._set_disclaimer_cookie()
        logger.info("‚úÖ Scraper Ultimate inicializado correctamente")

    def _set_disclaimer_cookie(self):
        """Configura la cookie de disclaimer"""
        try:
            self.session.cookies.set(
                'disclaimer_accepted',
                'true',
                domain='.platinsport.com',
                path='/',
                secure=True
            )
            logger.debug("‚úÖ Cookie configurada")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error configurando cookie: {e}")

    def generate_key(self) -> str:
        """Genera la key de acceso basada en la fecha actual"""
        today = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        key_string = today + "PLATINSPORT"
        key = base64.b64encode(key_string.encode()).decode()
        return key

    def validate_acestream_url(self, url: str) -> bool:
        """Valida formato de URL Acestream (acestream:// + 40 hex chars)"""
        if not url:
            return False
        pattern = r'^acestream://[a-f0-9]{40}$'
        return bool(re.match(pattern, url, re.IGNORECASE))

    def build_flag_icon_url(self, code: str, variant: str = '4x3') -> str:
        """Construye la URL del SVG de bandera exactamente como en la web (flag-icons)."""
        code = (code or '').strip().lower()
        if not code:
            return ''
        return f"https://www.platinsport.com/style/flag-icons-main/flags/{variant}/{code}.svg"

    def extract_language_flag_code_from_anchor(self, a_tag) -> Optional[str]:
        """Extrae el c√≥digo ISO (fi-xx) del dentro del enlace."""
        if not a_tag:
            return None
        span = a_tag.find('span', class_=lambda x: x and 'fi' in x)
        if not span:
            return None
        for c in (span.get('class') or []):
            if isinstance(c, str) and c.startswith('fi-') and len(c) == 5:
                return c.split('-', 1)[1]
        return None

    def _make_request(self, url: str, timeout: int = 20) -> Optional[requests.Response]:
        """Hace request con retry logic y deja que requests maneje la compresi√≥n."""
        for attempt in range(self.retry_count):
            try:
                response = self.session.get(url, timeout=timeout, allow_redirects=True)
                response.raise_for_status()

                # Forzar encoding UTF-8 si requests no lo detecta bien
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'

                return response

            except requests.exceptions.Timeout:
                logger.warning(f"‚è±Ô∏è Timeout en intento {attempt + 1}/{self.retry_count}")
                if attempt < self.retry_count - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))

            except requests.exceptions.HTTPError as e:
                logger.error(f"‚ùå HTTP Error {e.response.status_code}: {url}")
                return None

            except requests.exceptions.ConnectionError:
                logger.warning(f"üîå Error de conexi√≥n en intento {attempt + 1}/{self.retry_count}")
                if attempt < self.retry_count - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))

            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Request error: {e}")
                return None

        logger.error(f"‚ùå Fall√≥ despu√©s de {self.retry_count} intentos: {url}")
        return None

    def normalize_team_name(self, name: str) -> str:
        """Normaliza nombres de equipos para matching"""
        if not name:
            return ""
        # Eliminar espacios extras y normalizar
        name = re.sub(r'\s+', ' ', name).strip()
        # Normalizar caracteres especiales comunes
        replacements = {
            '‚Äô': "'",
            '‚Äò': "'",
            '‚Äú': '"',
            '‚Äù': '"',
            '‚Äî': '-',
            '‚Äì': '-'
        }
        for old, new in replacements.items():
            name = name.replace(old, new)
        return name

    def extract_events_from_main_page(self) -> List[Dict]:
        """Extrae eventos de la p√°gina principal"""
        logger.info("üîç Obteniendo p√°gina principal...")
        response = self._make_request(self.base_url)
        if not response:
            logger.error("‚ùå No se pudo obtener la p√°gina principal")
            return []

        soup = BeautifulSoup(response.text, 'lxml')
        events = []

        # Buscar todos los elementos que indican eventos
        time_elements = soup.find_all('time')
        logger.info(f"‚úÖ Encontrados {len(time_elements)} elementos <time> en la p√°gina principal")

        # Debug: verificar contenido si no hay ninguno
        if len(time_elements) == 0:
            logger.warning(f"‚ö†Ô∏è No se encontraron elementos <time>. Longitud HTML: {len(response.text)}")
            logger.warning(f"‚ö†Ô∏è Primeros 500 chars: {response.text[:500]}")

        for time_elem in time_elements:
            try:
                # Navegar hacia arriba para encontrar el contenedor del evento
                container = None
                current = time_elem
                for _ in range(5):  # M√°ximo 5 niveles hacia arriba
                    current = current.parent
                    if not current:
                        break
                    # Verificar si tiene un bot√≥n PLAY
                    play_link = current.find('a', href=re.compile(r'javascript:go'))
                    if play_link:
                        container = current
                        break

                if not container:
                    continue

                # Extraer informaci√≥n del evento
                datetime_str = time_elem.get('datetime', '')

                # Buscar nombres de equipos
                team_spans = container.find_all(
                    'span',
                    style=lambda x: x and 'font-size: 12px' in x and 'color: #fff' in x
                )

                teams = []
                for span in team_spans:
                    team_name = span.get_text(strip=True)
                    if team_name and team_name not in ['VS', '']:
                        teams.append(self.normalize_team_name(team_name))

                if len(teams) >= 2:
                    team1 = teams[0]
                    team2 = teams[1]
                elif len(teams) == 1:
                    team1 = teams[0]
                    team2 = "TBD"
                else:
                    continue  # Skip si no hay nombres de equipos

                # Extraer logos/banderas de equipos (si existen)
                img_tags = container.find_all('img') if container else []
                team_logo_urls = [img.get('src') for img in img_tags if img.get('src')]
                team1_logo_url = team_logo_urls[0] if len(team_logo_urls) >= 1 else ''
                team2_logo_url = team_logo_urls[1] if len(team_logo_urls) >= 2 else ''

                # Formatear hora
                match_time = "Unknown"
                if datetime_str:
                    try:
                        dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                        match_time = dt.strftime('%H:%M UTC')
                    except ValueError:
                        match_time = datetime_str

                # TODO: aqu√≠ seguir√≠a tu l√≥gica para mapear a source-list.php,
                # generar M3U, etc. Dejo el esqueleto:
                event = {
                    "team1": team1,
                    "team2": team2,
                    "team1_logo_url": team1_logo_url,
                    "team2_logo_url": team2_logo_url,
                    "datetime": datetime_str,
                    "match_time": match_time,
                }
                events.append(event)

            except Exception as e:
                logger.error(f"‚ùå Error procesando evento: {e}")

        logger.info(f"üìä Eventos v√°lidos extra√≠dos: {len(events)}")
        return events


def main():
    logger.info("=" * 80)
    logger.info(" üèÜ PLATINSPORT ULTIMATE SCRAPER üèÜ")
    logger.info("=" * 80)

    scraper = UltimatePlatinSportScraper()

    logger.info("=" * 80)
    logger.info("üèÜ PLATINSPORT ULTIMATE SCRAPER")
    now_utc = datetime.now(timezone.utc)
    logger.info(f"üìÖ Fecha: {now_utc.strftime('%Y-%m-%d %H:%M:%S')} UTC")
    key = scraper.generate_key()
    logger.info(f"üîë Key: {key}")
    logger.info("=" * 80)

    try:
        events = scraper.extract_events_from_main_page()
        if not events:
            logger.error("‚ùå No se encontraron eventos")
            raise RuntimeError("No events found")

        # Aqu√≠ ir√≠a tu l√≥gica de procesar events y generar el M3U final.
        # Por ahora solo log.
        for e in events:
            logger.info(f"‚öΩ {e['match_time']} - {e['team1']} vs {e['team2']}")

    except Exception as e:
        logger.error("‚ùå Error durante el scraping")
        logger.error(e)
        raise


if __name__ == "__main__":
    main()
