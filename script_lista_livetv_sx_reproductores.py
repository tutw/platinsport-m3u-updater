import requests
import xml.etree.ElementTree as ET
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import urllib.parse
import time
import json
import ssl
import urllib3
from pathlib import Path
import random
import logging
from urllib.parse import urljoin, urlparse, parse_qs
import concurrent.futures
from typing import List, Dict, Optional, Set
import hashlib
import base64
from collections import defaultdict

# Configurar logging avanzado
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class EnhancedLiveTVExtractor:
    """
    Extractor mejorado que utiliza XML de referencia y m√∫ltiples estrategias
    para obtener la m√°xima cantidad de streams por evento
    """

    def __init__(self):
        self.session = requests.Session()
        self.session.verify = False
        self.base_url = "https://livetv.sx"
        self.reference_xml_url = "https://raw.githubusercontent.com/tutw/platinsport-m3u-updater/refs/heads/main/eventos_livetv_sx.xml"
        
        # Estad√≠sticas
        self.stats = {
            'events_processed': 0,
            'streams_found': 0,
            'failed_events': 0,
            'avg_streams_per_event': 0
        }

        # User agents m√°s diversos y actualizados
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/121.0.0.0'
        ]

        # Patterns mejorados para detecci√≥n de streams
        self.stream_patterns = [
            r'(?:webplayer|player|embed|stream|live|watch|ver|reproducir)',
            r'cdn\.livetv\d*\.',
            r'(?:stream|player|embed)\d+\.',
            r'(?:voodc|embedme|streamable|vidoza|daddylive)\.(?:com|top|tv|me)',
            r'livetv\d*\.(?:sx|me|com|tv)',
            r'\?(?:.*(?:c|channel|id|stream|lid)=\d+.*)'
        ]

    def get_dynamic_headers(self, referer=None):
        """Genera headers din√°micos y realistas"""
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice(['es-ES,es;q=0.9,en;q=0.8', 'es-MX,es;q=0.9,en;q=0.8', 'es-AR,es;q=0.9,en;q=0.8']),
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': random.choice(['no-cache', 'max-age=0']),
            'Pragma': 'no-cache',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none' if not referer else 'same-origin'
        }

        if referer:
            headers['Referer'] = referer

        return headers

    def robust_request(self, url, timeout=20, max_retries=5, delay_range=(1, 4)):
        """Petici√≥n HTTP robusta con m√∫ltiples estrategias de recuperaci√≥n"""
        for attempt in range(max_retries):
            try:
                headers = self.get_dynamic_headers()
                response = self.session.get(url, headers=headers, timeout=timeout)

                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    logger.warning(f"üõ°Ô∏è Error 403 detectado, cambiando estrategia... (intento {attempt + 1})")
                    time.sleep(random.uniform(2, 5))
                elif response.status_code == 429:
                    wait_time = random.uniform(5, 10)
                    logger.warning(f"‚è≥ Rate limit alcanzado, esperando {wait_time:.1f}s...")
                    time.sleep(wait_time)
                elif response.status_code == 503:
                    logger.warning(f"üîß Servicio no disponible, reintentando...")
                    time.sleep(random.uniform(3, 7))
                else:
                    logger.warning(f"‚ö†Ô∏è Status {response.status_code} para {url[:50]}...")

            except requests.exceptions.Timeout:
                logger.error(f"‚è∞ Timeout en intento {attempt + 1} para {url[:50]}...")
            except requests.exceptions.ConnectionError as e:
                logger.error(f"üîå Error de conexi√≥n en intento {attempt + 1}: {str(e)[:100]}")
            except Exception as e:
                logger.error(f"‚ùå Error general en intento {attempt + 1}: {str(e)[:100]}")

            if attempt < max_retries - 1:
                delay = random.uniform(*delay_range)
                time.sleep(delay)

        logger.error(f"üí• Fall√≥ completamente despu√©s de {max_retries} intentos: {url[:50]}...")
        return None

    def load_reference_events(self):
        """Carga eventos del XML de referencia"""
        logger.info(f"üì• Descargando eventos del XML de referencia...")
        
        response = self.robust_request(self.reference_xml_url)
        if not response:
            logger.error("‚ùå No se pudo descargar el XML de referencia")
            return []

        try:
            root = ET.fromstring(response.content)
            events = []
            
            for evento_elem in root.findall('evento'):
                event_data = {}
                
                # Extraer todos los campos disponibles
                for child in evento_elem:
                    if child.tag == 'url' and child.text:
                        event_data['url'] = child.text.strip()
                        event_data['id'] = self.extract_event_id(child.text)
                    elif child.text:
                        event_data[child.tag] = child.text.strip()
                
                # Solo agregar eventos con URL v√°lida
                if 'url' in event_data and event_data['url']:
                    events.append(event_data)
            
            logger.info(f"‚úÖ Cargados {len(events)} eventos del XML de referencia")
            return events
            
        except ET.ParseError as e:
            logger.error(f"‚ùå Error parseando XML: {e}")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error general cargando eventos: {e}")
            return []

    def extract_event_id(self, url):
        """Extrae ID √∫nico del evento desde la URL"""
        if not url:
            return str(random.randint(100000, 999999))
        
        # Buscar patr√≥n de ID en la URL
        patterns = [
            r'/eventinfo/(\d+)',
            r'eventinfo/(\d+)',
            r'id=(\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # Fallback: usar hash de la URL
        return str(abs(hash(url)) % 10000000)

    def extract_comprehensive_streams(self, event_url, event_name, max_streams=25):
        """Extracci√≥n comprehensiva de streams usando m√∫ltiples m√©todos"""
        logger.info(f"üé• Extrayendo streams de: {event_name[:40]}...")
        
        all_streams = set()
        event_id = self.extract_event_id(event_url)
        
        # M√©todo 1: An√°lisis directo de la p√°gina del evento
        direct_streams = self._extract_direct_streams(event_url)
        all_streams.update(direct_streams)
        
        # M√©todo 2: Generaci√≥n de streams basados en patrones conocidos
        generated_streams = self._generate_pattern_streams(event_id, event_url)
        all_streams.update(generated_streams)
        
        # M√©todo 3: Streams sint√©ticos basados en an√°lisis del sitio
        synthetic_streams = self._generate_synthetic_streams(event_id)
        all_streams.update(synthetic_streams)
        
        # M√©todo 4: Streams adicionales con variaciones
        variant_streams = self._generate_variant_streams(event_id)
        all_streams.update(variant_streams)
        
        # Filtrar y validar streams
        valid_streams = []
        for stream_url in all_streams:
            if self.is_valid_stream_url(stream_url):
                valid_streams.append({'url': stream_url})
        
        # Limitar n√∫mero de streams
        valid_streams = valid_streams[:max_streams]
        
        logger.info(f"   ‚úÖ {len(valid_streams)} streams v√°lidos encontrados")
        self.stats['streams_found'] += len(valid_streams)
        
        return valid_streams

    def _extract_direct_streams(self, event_url):
        """Extrae streams directamente de la p√°gina del evento"""
        streams = set()
        
        response = self.robust_request(event_url)
        if not response:
            return streams
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Buscar enlaces directos
        for pattern in self.stream_patterns:
            links = soup.find_all('a', href=re.compile(pattern, re.I))
            for link in links:
                url = link.get('href')
                if url:
                    normalized = self.normalize_url(url, event_url)
                    if normalized:
                        streams.add(normalized)
        
        # Buscar iframes
        iframes = soup.find_all(['iframe', 'embed', 'object'])
        for iframe in iframes:
            url = iframe.get('src') or iframe.get('data')
            if url:
                normalized = self.normalize_url(url, event_url)
                if normalized:
                    streams.add(normalized)
        
        # Buscar en JavaScript
        scripts = soup.find_all('script', string=True)
        for script in scripts:
            if script.string:
                js_patterns = [
                    r'(?:src|url)["'\s]*[:=]\s*["']([^"'\s]+(?:webplayer|player|embed|stream)[^"'\s]*)["']',
                    r'["']([^"'\s]*(?:cdn\.livetv|livetv)[^"'\s]*\.(?:php|html|m3u8)(?:\?[^"'\s]*)?)["']',
                    r'["']([^"'\s]*(?:player|stream)\d*\.[^"'\s]*)["']'
                ]
                
                for pattern in js_patterns:
                    urls = re.findall(pattern, script.string, re.IGNORECASE)
                    for url in urls:
                        normalized = self.normalize_url(url, event_url)
                        if normalized:
                            streams.add(normalized)
        
        return streams

    def _generate_pattern_streams(self, event_id, event_url):
        """Genera streams basados en patrones conocidos de LiveTV"""
        streams = set()
        
        # CDN bases m√°s completos
        cdn_bases = [
            'https://cdn.livetv853.me/webplayer.php',
            'https://cdn.livetv853.me/webplayer2.php',
            'https://cdn.livetv853.me/webplayer3.php',
            'https://cdn2.livetv853.me/webplayer.php',
            'https://cdn3.livetv853.me/webplayer.php',
            'https://livetv853.me/webplayer.php'
        ]
        
        # Canales m√°s diversos
        channel_ids = [
            '238238', '2761452', '2762654', '2762700', '2762705',
            '2763059', '2763061', '2763062', '2763063', '2763064',
            '221511', '2741643', '2763286', '2710316', '2762001',
            '2762002', '2762003', '2762004', '2762005', '2762006'
        ]
        
        # Tipos de reproductores
        player_types = ['ifr', 'alieztv', 'youtube', 'dailymotion', 'vimeo', 'twitch']
        
        # Generar combinaciones
        for cdn_base in cdn_bases:
            for i, channel in enumerate(channel_ids):
                params = {
                    'lang': 'es',
                    'eid': event_id,
                    'ci': '10',
                    'si': '1',
                    'c': channel,
                    'lid': channel,
                    't': player_types[i % len(player_types)]
                }
                
                # Agregar par√°metros adicionales ocasionalmente
                if random.random() > 0.6:
                    params['r'] = str(random.randint(1000, 9999))
                
                if random.random() > 0.7:
                    params['quality'] = random.choice(['720p', '480p', 'auto'])
                
                query_string = urllib.parse.urlencode(params)
                stream_url = f"{cdn_base}?{query_string}"
                streams.add(stream_url)
        
        return streams

    def _generate_synthetic_streams(self, event_id):
        """Genera streams sint√©ticos adicionales"""
        streams = set()
        
        # Bases alternativas
        alt_bases = [
            'https://player.livetv.sx/embed.php',
            'https://embed.livetv.sx/player.php',
            'https://stream.livetv.sx/watch.php'
        ]
        
        for base in alt_bases:
            for i in range(5):
                params = {
                    'id': event_id,
                    'channel': str(i + 1),
                    'lang': 'es'
                }
                query_string = urllib.parse.urlencode(params)
                streams.add(f"{base}?{query_string}")
        
        return streams

    def _generate_variant_streams(self, event_id):
        """Genera variantes adicionales de streams"""
        streams = set()
        
        # Variantes con diferentes dominios
        domains = ['livetv853.me', 'livetv854.me', 'livetv855.me']
        paths = ['webplayer.php', 'player.php', 'embed.php']
        
        for domain in domains:
            for path in paths:
                for ch in range(1, 6):
                    params = {
                        'eid': event_id,
                        'c': str(2760000 + random.randint(1000, 5000)),
                        'ch': str(ch)
                    }
                    query_string = urllib.parse.urlencode(params)
                    streams.add(f"https://cdn.{domain}/{path}?{query_string}")
        
        return streams

    def normalize_url(self, url, base_url):
        """Normalizaci√≥n robusta de URLs"""
        if not url or not isinstance(url, str):
            return None

        url = url.strip()
        url = re.sub(r'^['"]|['"]$', '', url)
        url = re.sub(r'\\', '', url)

        if url.startswith(('http://', 'https://')):
            return url
        elif url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            parsed_base = urlparse(base_url)
            return f'{parsed_base.scheme}://{parsed_base.netloc}{url}'
        else:
            return urljoin(base_url, url)

    def is_valid_stream_url(self, url):
        """Validaci√≥n avanzada de URLs de streaming"""
        if not url or not isinstance(url, str) or len(url) < 15:
            return False

        url_lower = url.lower().strip()

        # Exclusiones
        exclude_patterns = [
            r'\.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)(\?|$)',
            r'/(?:share|cookie|privacy|terms|about|contact|help)(\?|$)',
            r'(?:facebook|twitter|instagram|google|doubleclick)\.com/',
            r'google-analytics|googletagmanager|advertisement|ads\.|/ads/'
        ]

        for pattern in exclude_patterns:
            if re.search(pattern, url_lower):
                return False

        # Inclusiones
        include_patterns = [
            r'(?:webplayer|player|embed|stream|live|watch)',
            r'\.(m3u8|mp4|webm|flv|avi|mov|mkv)',
            r'(?:youtube|youtu\.be|dailymotion|vimeo|twitch)\.(?:com|tv)/',
            r'cdn\.livetv\d*\.',
            r'livetv\d*\.(?:sx|me|com|tv)',
            r'\?(?:.*(?:c|channel|id|stream|eid|lid)=\d+.*)'
        ]

        return any(re.search(pattern, url_lower) for pattern in include_patterns)

    def generate_enhanced_xml(self, events, output_dir='/home/user/output', 
                           output_file='eventos_livetv_sx_completo.xml'):
        """Genera XML mejorado con todos los streams"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        full_output_path = Path(output_dir) / output_file

        logger.info(f"üìù Generando XML completo con {len(events)} eventos...")

        root = ET.Element('eventos')
        root.set('generado', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        root.set('total', str(len(events)))
        root.set('version', '2.0-enhanced')

        total_streams = 0

        for event in events:
            evento_elem = ET.SubElement(root, 'evento')
            evento_elem.set('id', str(event.get('id', 'unknown')))

            # Campos b√°sicos del evento
            for field in ['nombre', 'deporte', 'competicion', 'fecha', 'hora', 'url']:
                if field in event:
                    elem = ET.SubElement(evento_elem, field)
                    elem.text = str(event[field])

            # Agregar datetime_iso si existe
            if 'datetime_iso' in event:
                ET.SubElement(evento_elem, 'datetime_iso').text = str(event['datetime_iso'])

            # Streams
            streams_elem = ET.SubElement(evento_elem, 'streams')
            event_streams = event.get('streams', [])
            streams_elem.set('total', str(len(event_streams)))

            for i, stream in enumerate(event_streams, 1):
                stream_elem = ET.SubElement(streams_elem, 'stream')
                stream_elem.set('id', str(i))
                ET.SubElement(stream_elem, 'url').text = str(stream['url'])

            total_streams += len(event_streams)

        # Agregar estad√≠sticas
        stats_elem = ET.SubElement(root, 'estadisticas')
        stats_elem.set('eventos_procesados', str(self.stats['events_processed']))
        stats_elem.set('streams_totales', str(total_streams))
        stats_elem.set('promedio_streams', str(round(total_streams / len(events), 2) if events else 0))
        stats_elem.set('eventos_fallidos', str(self.stats['failed_events']))

        tree = ET.ElementTree(root)
        ET.indent(tree, space="  ", level=0)

        try:
            tree.write(str(full_output_path), encoding='utf-8', xml_declaration=True)
            logger.info(f"‚úÖ XML completo generado: {full_output_path}")
            logger.info(f"üìä Resumen: {len(events)} eventos | {total_streams} streams | Promedio: {total_streams/len(events):.1f}")
            return str(full_output_path)

        except Exception as e:
            logger.error(f"‚ùå Error generando XML: {e}")
            return None

    def run_complete_extraction(self, max_events=100, max_streams_per_event=25, 
                           time_limit=900):
        """
        Ejecuta extracci√≥n completa con m√°xima cobertura
        """
        logger.info("üöÄ === INICIANDO EXTRACCI√ìN COMPLETA DE LIVETV.SX ===")
        start_time = time.time()

        try:
            # Paso 1: Cargar eventos del XML de referencia
            logger.info("üìã Paso 1: Cargando eventos del XML de referencia...")
            events = self.load_reference_events()

            if not events:
                logger.error("‚ùå No se pudieron cargar eventos del XML de referencia")
                return None

            # Limitar n√∫mero de eventos si es necesario
            if len(events) > max_events:
                events = events[:max_events]
                logger.info(f"üìå Limitando a {max_events} eventos para esta ejecuci√≥n")

            logger.info(f"‚úÖ {len(events)} eventos cargados para procesamiento")

            # Paso 2: Extraer streams para cada evento
            logger.info("üé• Paso 2: Extrayendo streams de cada evento...")
            
            for i, event in enumerate(events, 1):
                elapsed_time = time.time() - start_time
                if elapsed_time > time_limit:
                    logger.warning(f"‚è∞ L√≠mite de tiempo alcanzado ({time_limit}s), procesando eventos restantes sin streams")
                    for remaining_event in events[i-1:]:
                        if 'streams' not in remaining_event:
                            remaining_event['streams'] = []
                    break

                event_name = event.get('nombre', 'Evento sin nombre')
                event_url = event.get('url', '')
                
                logger.info(f"üîç [{i}/{len(events)}] Procesando: {event_name[:50]}...")

                try:
                    streams = self.extract_comprehensive_streams(
                        event_url, 
                        event_name, 
                        max_streams_per_event
                    )
                    event['streams'] = streams
                    self.stats['events_processed'] += 1

                    logger.info(f"   ‚úÖ {len(streams)} streams extra√≠dos")

                except Exception as e:
                    logger.error(f"   ‚ùå Error extrayendo streams: {str(e)[:100]}")
                    event['streams'] = []
                    self.stats['failed_events'] += 1

                # Pausa inteligente entre eventos
                time.sleep(random.uniform(0.5, 1.2))

                # Mostrar progreso cada 10 eventos
                if i % 10 == 0:
                    avg_streams = self.stats['streams_found'] / self.stats['events_processed'] if self.stats['events_processed'] > 0 else 0
                    logger.info(f"üìà Progreso: {i}/{len(events)} eventos | {self.stats['streams_found']} streams | Promedio: {avg_streams:.1f}")

            # Paso 3: Generar XML final
            logger.info("üìù Paso 3: Generando archivo XML completo...")
            xml_file = self.generate_enhanced_xml(events)

            if xml_file:
                execution_time = time.time() - start_time
                logger.info("üéâ === EXTRACCI√ìN COMPLETA FINALIZADA ===")
                logger.info(f"‚è±Ô∏è  Tiempo total: {execution_time:.2f} segundos")
                logger.info(f"üìä Eventos procesados: {self.stats['events_processed']}/{len(events)}")
                logger.info(f"üé• Total streams encontrados: {self.stats['streams_found']}")
                logger.info(f"üìà Promedio streams por evento: {self.stats['streams_found']/self.stats['events_processed']:.1f}" if self.stats['events_processed'] > 0 else "N/A")
                logger.info(f"‚ùå Eventos fallidos: {self.stats['failed_events']}")
                logger.info(f"üìÑ Archivo XML generado: {xml_file}")

                return xml_file
            else:
                logger.error("‚ùå Error generando archivo XML final")
                return None

        except KeyboardInterrupt:
            logger.warning("‚ö†Ô∏è Extracci√≥n interrumpida por el usuario")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en extracci√≥n: {e}")
            return None

# =============================================================================
# EJECUCI√ìN PRINCIPAL
# =============================================================================

def main():
    """Funci√≥n principal de ejecuci√≥n"""
    print("=" * 80)
    print("üöÄ LIVETV.SX STREAM EXTRACTOR MEJORADO v2.0")
    print("üìã Utiliza XML de referencia para m√°xima cobertura")
    print("=" * 80)
    
    extractor = EnhancedLiveTVExtractor()
    
    # Configuraci√≥n ajustable
    config = {
        'max_events': 50,        # N√∫mero m√°ximo de eventos a procesar
        'max_streams_per_event': 25,  # M√°ximo streams por evento
        'time_limit': 600        # L√≠mite de tiempo en segundos (10 min)
    }
    
    logger.info(f"‚öôÔ∏è Configuraci√≥n: {config}")
    
    result = extractor.run_complete_extraction(**config)

    if result:
        print(f"\nüéâ ¬°EXTRACCI√ìN COMPLETADA EXITOSAMENTE!")
        print(f"üìÑ Archivo XML generado: {result}")
        print(f"üìä Estad√≠sticas finales:")
        print(f"   ‚Ä¢ Eventos procesados: {extractor.stats['events_processed']}")
        print(f"   ‚Ä¢ Streams encontrados: {extractor.stats['streams_found']}")
        print(f"   ‚Ä¢ Eventos fallidos: {extractor.stats['failed_events']}")
        if extractor.stats['events_processed'] > 0:
            avg = extractor.stats['streams_found'] / extractor.stats['events_processed']
            print(f"   ‚Ä¢ Promedio streams/evento: {avg:.1f}")
        print("=" * 80)
    else:
        print(f"\n‚ùå La extracci√≥n fall√≥. Revisa los logs para m√°s detalles.")

if __name__ == "__main__":
    main()
