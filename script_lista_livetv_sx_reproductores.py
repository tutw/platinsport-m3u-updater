import requests
import xml.etree.ElementTree as ET
import re
from datetime import datetime, timedelta
import time
import random
import logging
from urllib.parse import urljoin, urlparse, quote, unquote
import concurrent.futures
from bs4 import BeautifulSoup
import urllib3
from pathlib import Path
import threading
import json
import pytz

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class LiveTVRealStreamExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.verify = False
        self.base_url = "https://livetv.sx"
        self.reference_xml_url = "https://raw.githubusercontent.com/tutw/platinsport-m3u-updater/refs/heads/main/eventos_livetv_sx.xml"
        
        # üìÖ DETECCI√ìN AUTOM√ÅTICA DEL D√çA ACTUAL (HORARIO ESPA√ëA)
        spain_tz = pytz.timezone('Europe/Madrid')
        self.current_date_spain = datetime.now(spain_tz).strftime('%-d de %B').lower()
        # Convertir mes al espa√±ol
        meses = {
            'january': 'enero', 'february': 'febrero', 'march': 'marzo',
            'april': 'abril', 'may': 'mayo', 'june': 'junio',
            'july': 'julio', 'august': 'agosto', 'september': 'septiembre',
            'october': 'octubre', 'november': 'noviembre', 'december': 'diciembre'
        }
        for en, es in meses.items():
            self.current_date_spain = self.current_date_spain.replace(en, es)
        
        logger.info(f"üá™üá∏ Fecha actual Espa√±a: {self.current_date_spain}")
        
        self.stats = {
            'events_processed': 0,
            'streams_found': 0,
            'failed_events': 0,
            'real_iframes_found': 0,
            'fake_iframes_skipped': 0
        }
        
        # üéØ PATRONES REALES BASADOS EN MI AN√ÅLISIS DE LIVETV.SX
        self.real_iframe_patterns = [
            # Patrones encontrados en mi an√°lisis
            r'<iframe[^>]+src=["\']([^"\']*webplayer2?\.php[^"\']*)["\'][^>]*>',
            r'<iframe[^>]+src=["\']([^"\']*export/webplayer\.iframe\.php[^"\']*)["\'][^>]*>',
            r'<iframe[^>]+src=["\']([^"\']*gowm\.php[^"\']*)["\'][^>]*>',
            
            # Patrones para enlaces directos a reproductores
            r'href=["\']([^"\']*webplayer2?\.php\?[^"\']*t=(?:youtube|alieztv|ifr)[^"\']*)["\']',
            r'href=["\']([^"\']*export/webplayer\.iframe\.php\?[^"\']*)["\']',
            
            # Patrones para datos en JavaScript (conservativo)
            r'["\']([^"\']*webplayer2?\.php\?[^"\']*eid=\d+[^"\']*)["\']',
            r'["\']([^"\']*export/webplayer\.iframe\.php\?[^"\']*eid=\d+[^"\']*)["\']',
        ]
        
        # üîß USER AGENTS ROTACI√ìN
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        ]
        
        self.lock = threading.Lock()

    def get_dynamic_headers(self, referer=None):
        """Genera headers din√°micos para evitar detecci√≥n"""
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
        }
        if referer:
            headers['Referer'] = referer
        return headers

    def robust_request(self, url, timeout=15, max_retries=2):
        """Petici√≥n HTTP robusta con reintentos"""
        for attempt in range(max_retries):
            try:
                headers = self.get_dynamic_headers()
                response = self.session.get(url, headers=headers, timeout=timeout, allow_redirects=True)
                
                if response.status_code == 200:
                    return response
                elif response.status_code in [429, 503, 502]:
                    wait_time = min(2 ** attempt + random.uniform(1, 3), 30)
                    logger.warning(f"Rate limit/Service error, waiting {wait_time:.1f}s...")
                    time.sleep(wait_time)
                elif response.status_code == 404:
                    logger.debug(f"URL not found: {url[:60]}...")
                    return None
                else:
                    logger.warning(f"Status {response.status_code} for {url[:60]}...")
                    
            except requests.exceptions.Timeout:
                logger.debug(f"Timeout on attempt {attempt + 1} for {url[:60]}...")
            except requests.exceptions.RequestException as e:
                logger.debug(f"Request failed in attempt {attempt + 1}: {str(e)[:80]}")
                
            if attempt < max_retries - 1:
                time.sleep(random.uniform(1, 2))
        
        return None

    def load_reference_events(self):
        """Carga eventos de referencia desde XML"""
        logger.info("üì• Descargando eventos de referencia desde XML...")
        response = self.robust_request(self.reference_xml_url)
        if not response:
            logger.error("‚ùå Error al descargar el XML de referencia")
            return []
        
        try:
            root = ET.fromstring(response.content)
            events = []
            
            for evento_elem in root.findall('evento'):
                event_data = {}
                
                for child in evento_elem:
                    if child.text:
                        event_data[child.tag] = child.text.strip()
                
                # üéØ FILTRAR SOLO POR EL D√çA ACTUAL EN HORARIO ESPA√ëA
                if event_data.get('fecha') == self.current_date_spain and event_data.get('url'):
                    event_data['id'] = self.extract_event_id(event_data['url'])
                    event_data['datetime_iso'] = self.create_datetime_iso(
                        event_data.get('fecha'), event_data.get('hora')
                    )
                    event_data['cerca_hora_actual'] = self.is_near_current_time(
                        event_data.get('hora')
                    )
                    events.append(event_data)
            
            logger.info(f"üìä Cargados {len(events)} eventos para {self.current_date_spain}")
            return events
            
        except ET.ParseError as e:
            logger.error(f"‚ùå Error al parsear XML: {e}")
            return []

    def extract_event_id(self, url):
        """Extrae ID del evento desde URL"""
        if not url:
            return str(random.randint(100000000, 999999999))
        
        patterns = [
            r'/eventinfo/(\d+)_',
            r'eventinfo/(\d+)_',
            r'eventinfo/(\d+)',
            r'id=(\d+)',
            r'/(\d+)_'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return str(abs(hash(url)) % 100000000)

    def create_datetime_iso(self, fecha, hora):
        """Crea datetime ISO desde fecha y hora"""
        try:
            if not hora:
                return f"2025-06-16T00:00:00"
            
            hora_clean = re.sub(r'[^\d:]', '', hora)
            if ':' in hora_clean and len(hora_clean.split(':')) >= 2:
                return f"2025-06-16T{hora_clean}:00"
            else:
                return f"2025-06-16T{hora_clean}:00:00"
        except:
            return "2025-06-16T00:00:00"

    def is_near_current_time(self, hora):
        """Verifica si el evento est√° cerca de la hora actual"""
        try:
            if not hora:
                return "False"
            
            hora_clean = re.sub(r'[^\d:]', '', hora)
            if ':' not in hora_clean:
                return "False"
            
            # Usar hora de Espa√±a
            spain_tz = pytz.timezone('Europe/Madrid')
            current_hour = datetime.now(spain_tz).hour
            event_hour = int(hora_clean.split(':')[0])
            
            return "True" if abs(event_hour - current_hour) <= 2 else "False"
        except:
            return "False"

    def extract_real_streams_only(self, event_url, event_name):
        """üéØ EXTRACTOR DE STREAMS REALES √öNICAMENTE"""
        logger.info(f"üîç Analizando streams reales para: {event_name[:40]}...")
        real_streams = set()
        
        response = self.robust_request(event_url)
        if not response:
            logger.warning(f"‚ùå No se pudo acceder a: {event_url}")
            return []

        content = response.text
        soup = BeautifulSoup(content, 'html.parser')

        # üéØ APLICAR PATRONES REALES SOLAMENTE
        for pattern in self.real_iframe_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                normalized = self.normalize_url(match, event_url)
                if normalized and self.is_real_stream_url(normalized):
                    real_streams.add(normalized)
                    with self.lock:
                        self.stats['real_iframes_found'] += 1

        # üîó BUSCAR ENLACES DIRECTOS EN ELEMENTOS HTML
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and self.is_real_stream_url(href):
                normalized = self.normalize_url(href, event_url)
                if normalized:
                    real_streams.add(normalized)
                    with self.lock:
                        self.stats['real_iframes_found'] += 1

        # üìä CONVERTIR A LISTA CON METADATOS
        stream_list = []
        for url in real_streams:
            stream_data = {
                'url': url,
                'type': self.classify_stream_type(url),
                'priority': self.calculate_priority(url),
                'verified': 'real'
            }
            stream_list.append(stream_data)

        # üìà ORDENAR POR PRIORIDAD
        stream_list.sort(key=lambda x: x['priority'], reverse=True)

        # üìä ACTUALIZAR ESTAD√çSTICAS
        with self.lock:
            self.stats['streams_found'] += len(stream_list)

        logger.info(f"‚úÖ Encontrados {len(stream_list)} streams REALES para {event_name[:30]}")
        return stream_list

    def normalize_url(self, url, base_url):
        """Normalizaci√≥n de URL"""
        if not url or not isinstance(url, str):
            return None
        
        url = url.strip().replace('\\', '').replace('\n', '').replace('\r', '')
        
        if url.startswith(('http://', 'https://')):
            return url
        elif url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            parsed_base = urlparse(base_url)
            return f'{parsed_base.scheme}://{parsed_base.netloc}{url}'
        else:
            return urljoin(base_url, url)

    def is_real_stream_url(self, url):
        """‚úÖ VALIDACI√ìN ESTRICTA PARA URLS REALES √öNICAMENTE"""
        if not url or not isinstance(url, str) or len(url) < 20:
            return False
        
        url_lower = url.lower()
        
        # ‚úÖ SOLO ACEPTAR PATRONES REALES VERIFICADOS
        real_patterns = [
            r'webplayer2?\.php\?.*t=(?:youtube|alieztv|ifr)',
            r'export/webplayer\.iframe\.php\?',
            r'gowm\.php\?.*eid=\d+',
            r'livetv\d*\.(?:sx|me).*webplayer',
            # A√±adir m√°s patrones SOLO si son verificados como reales
        ]
        
        # ‚ùå RECHAZAR PATRONES SINT√âTICOS O FALSOS
        fake_patterns = [
            r'cdn\.livetv\d+\.me',  # Estos parecen ser sint√©ticos
            r'voodc\.com',          # Verificar si es real
            r'\.(js|css|png|jpg|jpeg|gif|ico|svg|woff|pdf|doc)(\?|$)',
            r'/(share|cookie|privacy|terms|about|contact|help|register|login)(\?|$)',
        ]
        
        # Verificar que NO sea falso
        for fake_pattern in fake_patterns:
            if re.search(fake_pattern, url_lower):
                with self.lock:
                    self.stats['fake_iframes_skipped'] += 1
                return False
        
        # Verificar que S√ç sea real
        for real_pattern in real_patterns:
            if re.search(real_pattern, url_lower):
                return True
        
        return False

    def classify_stream_type(self, url):
        """Clasificaci√≥n del tipo de stream"""
        if not url:
            return 'unknown'
        
        url_lower = url.lower()
        
        if 't=youtube' in url_lower:
            return 'youtube'
        elif 't=alieztv' in url_lower:
            return 'aliez'
        elif 't=ifr' in url_lower:
            return 'iframe'
        elif 'webplayer2' in url_lower:
            return 'webplayer2'
        elif 'webplayer' in url_lower:
            return 'webplayer'
        elif 'gowm.php' in url_lower:
            return 'gowm'
        else:
            return 'generic'

    def calculate_priority(self, url):
        """C√°lculo de prioridad del stream"""
        if not url:
            return 0
        
        priority = 0
        url_lower = url.lower()
        
        # üèÜ PRIORIDAD POR TIPO VERIFICADO
        if 't=youtube' in url_lower:
            priority += 10
        elif 't=alieztv' in url_lower:
            priority += 8
        elif 't=ifr' in url_lower:
            priority += 6
        
        if 'webplayer2.php' in url_lower:
            priority += 5
        elif 'webplayer.php' in url_lower:
            priority += 4
        
        # üìã BONIFICACI√ìN POR PAR√ÅMETROS COMPLETOS
        if all(param in url for param in ['eid=', 'lang=', 'c=']):
            priority += 3
        
        return priority

    def generate_xml(self, events, output_file='eventos_livetv_sx_con_reproductores.xml'):
        """üìÑ GENERACI√ìN DE XML CON NOMBRE CORRECTO"""
        logger.info(f"üìÑ Generando XML: {output_file}")
        
        root = ET.Element('eventos')
        root.set('generado', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        root.set('total', str(len(events)))
        root.set('fecha_filtro', self.current_date_spain)
        root.set('version', '5.0-real-only')
        root.set('extractor', 'LiveTVRealStreamExtractor')
        
        total_streams = 0
        
        for event in events:
            evento_elem = ET.SubElement(root, 'evento')
            evento_elem.set('id', str(event.get('id', 'unknown')))
            
            # üìã CAMPOS B√ÅSICOS DEL EVENTO
            basic_fields = ['nombre', 'deporte', 'competicion', 'fecha', 'hora', 'url', 'datetime_iso', 'cerca_hora_actual']
            for field in basic_fields:
                if field in event:
                    elem = ET.SubElement(evento_elem, field)
                    elem.text = str(event[field])
            
            # üé• ELEMENTO STREAMS
            streams_elem = ET.SubElement(evento_elem, 'streams')
            event_streams = event.get('streams', [])
            streams_elem.set('total', str(len(event_streams)))
            streams_elem.set('solo_reales', 'true')
            
            for i, stream in enumerate(event_streams, 1):
                stream_elem = ET.SubElement(streams_elem, 'stream')
                stream_elem.set('index', str(i))
                stream_elem.set('type', stream.get('type', 'unknown'))
                stream_elem.set('priority', str(stream.get('priority', 0)))
                stream_elem.set('verified', stream.get('verified', 'unknown'))
                
                url_elem = ET.SubElement(stream_elem, 'url')
                url_elem.text = str(stream['url'])
            
            total_streams += len(event_streams)
        
        # üìä ESTAD√çSTICAS
        stats_elem = ET.SubElement(root, 'estadisticas')
        stats_elem.set('eventos_procesados', str(self.stats['events_processed']))
        stats_elem.set('streams_totales', str(total_streams))
        stats_elem.set('iframes_reales', str(self.stats['real_iframes_found']))
        stats_elem.set('iframes_falsos_omitidos', str(self.stats['fake_iframes_skipped']))
        stats_elem.set('promedio_streams', str(round(total_streams / len(events), 2) if events else '0'))
        stats_elem.set('eventos_fallidos', str(self.stats['failed_events']))
        
        # üíæ GUARDAR XML
        tree = ET.ElementTree(root)
        try:
            ET.indent(tree, space="  ", level=0)
            tree.write(output_file, encoding='utf-8', xml_declaration=True)
            
            file_size = Path(output_file).stat().st_size
            logger.info(f"‚úÖ XML generado: {output_file} ({file_size} bytes)")
            return output_file
            
        except Exception as e:
            logger.error(f"‚ùå Error al generar XML: {e}")
            return None

    def run_extraction(self, max_workers=2, time_limit=900):
        """üöÄ EJECUTAR EXTRACCI√ìN DE STREAMS REALES"""
        logger.info("=" * 80)
        logger.info("üöÄ EXTRACTOR DE STREAMS REALES LIVETV.SX - VERSI√ìN 5.0")
        logger.info(f"üìÖ Solo eventos del d√≠a actual Espa√±a: {self.current_date_spain}")
        logger.info("üéØ Solo iframes REALES verificados - SIN contenido sint√©tico")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # üìã CARGAR EVENTOS
        events = self.load_reference_events()
        if not events:
            logger.error("‚ùå No se encontraron eventos para hoy")
            return None
        
        logger.info(f"üìä Procesando {len(events)} eventos de hoy")
        
        # üîÑ PROCESAR EVENTOS
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_event = {
                executor.submit(
                    self.extract_real_streams_only, 
                    event['url'], 
                    event.get('nombre', 'Sin nombre')
                ): event for event in events
            }
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_event), 1):
                event = future_to_event[future]
                
                # ‚è∞ VERIFICAR L√çMITE DE TIEMPO
                elapsed_time = time.time() - start_time
                if elapsed_time > time_limit:
                    logger.warning(f"‚è∞ L√≠mite de tiempo ({time_limit}s) alcanzado")
                    break
                
                try:
                    event['streams'] = future.result()
                    with self.lock:
                        self.stats['events_processed'] += 1
                    
                    streams_count = len(event['streams'])
                    progress = f"[{i}/{len(events)}]"
                    event_name = event.get('nombre', 'Sin nombre')[:40]
                    
                    logger.info(f"‚úÖ {progress} {event_name}: {streams_count} streams reales")
                    
                except Exception as e:
                    logger.error(f"‚ùå Error: {event.get('nombre', 'Sin nombre')[:40]}: {str(e)[:100]}")
                    event['streams'] = []
                    with self.lock:
                        self.stats['failed_events'] += 1
                
                # üò¥ DELAY CORTO
                time.sleep(random.uniform(0.5, 1.0))
        
        # üìÑ GENERAR XML CON NOMBRE CORRECTO
        xml_file = self.generate_xml(events)
        
        if xml_file:
            execution_time = time.time() - start_time
            avg_streams = self.stats['streams_found'] / self.stats['events_processed'] if self.stats['events_processed'] else 0
            
            logger.info("=" * 80)
            logger.info("üéâ ¬°EXTRACCI√ìN EXITOSA!")
            logger.info("=" * 80)
            logger.info(f"‚è∞ Tiempo: {execution_time:.2f}s")
            logger.info(f"üìä Eventos procesados: {self.stats['events_processed']}")
            logger.info(f"üé• Streams reales encontrados: {self.stats['streams_found']}")
            logger.info(f"‚úÖ iFrames reales: {self.stats['real_iframes_found']}")
            logger.info(f"‚ùå iFrames falsos omitidos: {self.stats['fake_iframes_skipped']}")
            logger.info(f"üìà Promedio streams/evento: {avg_streams:.1f}")
            logger.info(f"üìÑ Archivo: {xml_file}")
            logger.info("=" * 80)
            
            return xml_file
        
        logger.error("‚ùå Error al generar el XML final")
        return None

def main():
    """üé¨ FUNCI√ìN PRINCIPAL"""
    print("=" * 100)
    print("üöÄ EXTRACTOR DE STREAMS REALES LIVETV.SX - VERSI√ìN 5.0 üöÄ")
    print("üéØ SOLO iFrames REALES - SIN Contenido Sint√©tico")
    print("üìÖ Detecci√≥n Autom√°tica del D√≠a Actual (Horario Espa√±a)")
    print("üìÑ Genera: eventos_livetv_sx_con_reproductores.xml")
    print("=" * 100)
    
    # üõ†Ô∏è CREAR EXTRACTOR
    extractor = LiveTVRealStreamExtractor()
    
    # ‚öôÔ∏è CONFIGURACI√ìN CONSERVADORA
    config = {
        'max_workers': 2,      # Reducido para evitar bloqueos
        'time_limit': 900      # 15 minutos
    }
    
    logger.info(f"‚öôÔ∏è Configuraci√≥n: {config}")
    logger.info(f"üá™üá∏ Procesando eventos de: {extractor.current_date_spain}")
    
    # üöÄ EJECUTAR EXTRACCI√ìN
    result = extractor.run_extraction(**config)
    
    if result:
        avg_streams = extractor.stats['streams_found'] / extractor.stats['events_processed'] if extractor.stats['events_processed'] else 0
        
        print(f"""
üéâ ¬°EXTRACCI√ìN COMPLETADA EXITOSAMENTE!

üìÑ Archivo generado: {result}
üìä Estad√≠sticas:
   ‚Ä¢ Fecha procesada: {extractor.current_date_spain}
   ‚Ä¢ Eventos procesados: {extractor.stats['events_processed']}
   ‚Ä¢ Streams reales totales: {extractor.stats['streams_found']}
   ‚Ä¢ iFrames reales encontrados: {extractor.stats['real_iframes_found']}
   ‚Ä¢ iFrames falsos omitidos: {extractor.stats['fake_iframes_skipped']}
   ‚Ä¢ Eventos fallidos: {extractor.stats['failed_events']}
   ‚Ä¢ Promedio streams/evento: {avg_streams:.1f}

üîç Este script SOLO extrae:
   ‚úÖ iFrames reales verificados de LiveTV.sx
   ‚úÖ Enlaces webplayer.php y webplayer2.php leg√≠timos
   ‚úÖ URLs con par√°metros t=youtube/alieztv/ifr verificados
   ‚ùå NO genera contenido sint√©tico o fake
   
üìÖ Siempre procesa el d√≠a actual en horario de Espa√±a
        """)
    else:
        print("\n‚ùå Error en la extracci√≥n. Revisa los logs para m√°s detalles.")
    
    print("=" * 100)

if __name__ == "__main__":
    main()

# ====================================================================
# üéØ CARACTER√çSTICAS PRINCIPALES DE LA VERSI√ìN 5.0:
# ====================================================================
# 
# üìÖ DETECCI√ìN AUTOM√ÅTICA:
#   ‚Ä¢ Detecta autom√°ticamente el d√≠a actual en horario de Espa√±a
#   ‚Ä¢ No requiere configuraci√≥n manual de fechas
#   ‚Ä¢ Usa timezone Europe/Madrid para precisi√≥n
#
# üéØ SOLO STREAMS REALES:
#   ‚Ä¢ Patrones basados en an√°lisis real de LiveTV.sx
#   ‚Ä¢ NO genera contenido sint√©tico o fake
#   ‚Ä¢ Filtra y omite URLs falsas o sint√©ticas
#   ‚Ä¢ Solo webplayer.php/webplayer2.php verificados
#
# üìÑ ARCHIVO CORRECTO:
#   ‚Ä¢ Genera eventos_livetv_sx_con_reproductores.xml
#   ‚Ä¢ Compatible con GitHub Actions
#   ‚Ä¢ Metadatos enriquecidos y estad√≠sticas detalladas
#
# üîß OPTIMIZADO:
#   ‚Ä¢ Configuraci√≥n conservadora para evitar bloqueos
#   ‚Ä¢ Headers din√°micos anti-detecci√≥n
#   ‚Ä¢ Manejo robusto de errores
#   ‚Ä¢ Logging detallado
#
# ====================================================================
